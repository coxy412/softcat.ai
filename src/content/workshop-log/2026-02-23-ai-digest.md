---
title: "AI digest: Hardware wars heat up"
date: 2026-02-23
tags: [ai-news, digest, hardware, military-ai]
summary: "Taalas ditches GPUs for hardwired AI chips, Pentagon summons Anthropic CEO, and ByteDance cracks the reasoning code."
draft: false
---

This week's big theme is specialization. Everyone's realizing that general-purpose everything might not be the answer.

## Taalas wants to kill the GPU with 17,000 token/sec chips

Toronto startup [Taalas is ditching programmable GPUs](https://www.marktechpost.com/2026/02/22/taalas-is-replacing-programmable-gpus-with-hardwired-ai-chips-to-achieve-17000-tokens-per-second-for-ubiquitous-inference/) for hardwired AI chips that hit 17,000 tokens per second. Their bet is that flexibility is actually holding AI back, and custom silicon beats general-purpose every time. Bold move, but if they're right, this could reshape the entire inference game.

## Pentagon calls Anthropic CEO to the principal's office

Defense Secretary Pete Hegseth [summoned Anthropic CEO Dario Amodei](https://techcrunch.com/2026/02/23/defense-secretary-summons-anthropics-amodei-over-military-use-of-claude/) for a tense chat about Claude's military use. Hegseth is threatening to label Anthropic a "supply chain risk." This feels like the opening shot in a much bigger battle over AI companies working with the military.

## ByteDance maps AI reasoning like molecular bonds

ByteDance researchers figured out why [long chain-of-thought reasoning falls apart](https://www.marktechpost.com/2026/02/22/forget-keyword-imitation-bytedance-ai-maps-molecular-bonds-in-ai-reasoning-to-stabilize-long-chain-of-thought-performance-and-reinforcement-learning-rl-training/) and how to fix it. They're mapping reasoning patterns like molecular bonds instead of just copying keywords. Smart approach that could finally make multi-step reasoning reliable.

## Google says think harder, not longer

New Google research shows that [longer chain-of-thought isn't better](https://www.marktechpost.com/2026/02/21/a-new-google-ai-research-proposes-deep-thinking-ratio-to-improve-llm-accuracy-while-cutting-total-inference-costs-by-half/), deeper thinking is. Their "deep-thinking ratio" improves accuracy while cutting inference costs in half. Makes sense - quality over quantity has always been true for human thinking too.
